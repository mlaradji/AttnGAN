# AttnGAN implementation of AI-Platform text-to-image generation task
This task was designed to be used within Produvia's [`ai-platform`](https://github.com/produvia/ai-platform.git).

## AttnGAN (Python 3, Pytorch 1.0)

Pytorch implementation for reproducing AttnGAN results in the paper [AttnGAN: Fine-Grained Text to Image Generation
with Attentional Generative Adversarial Networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf) by Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He. (This work was performed when Tao was an intern with Microsoft Research). 

<img src="framework.png" width="900px" height="350px"/>

### MLFlow
To run this MLFlow task locally using `mlflow`, execute the following in the AttnGAN directory:
```
mlflow . [-e {endpoint}] [-P {parameter1}={value1} -P {parameter2}={value2} ...]
```

See the `MLproject` file for available endpoints and parameters.

To run the `birds` dataset example, execute:
```
mlflow . -e birds
```

This will download the `birds` dataset, train the DAMSME, train the AttnGAN, and then create images from the lines in `data/birds/example_filenames.txt`. Note that this is a large model and may require a GPU with more than 15 GB of RAM.

### Examples generated by AttnGAN [[Blog]](https://blogs.microsoft.com/ai/drawing-ai/)**

 bird example              |  coco example
:-------------------------:|:-------------------------:
![](https://github.com/taoxugit/AttnGAN/blob/master/example_bird.png)  |  ![](https://github.com/taoxugit/AttnGAN/blob/master/example_coco.png)


### Creating an API
[Evaluation code](eval) embedded into a callable containerized API is included in the `eval\` folder.

### Citing AttnGAN
If you find AttnGAN useful in your research, please consider citing:

```
@article{Tao18attngan,
  author    = {Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He},
  title     = {AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks},
  Year = {2018},
  booktitle = {{CVPR}}
}
```

### Reference

- [StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1710.10916) [[code]](https://github.com/hanzhanggit/StackGAN-v2)
- [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) [[code]](https://github.com/carpedm20/DCGAN-tensorflow)

### Non-MLFlow

#### Dependencies
The main requirements are `python>3.6` and `pytorch>1`.

MLFlow runtime dependencies are in `conda.yaml`, and will be automatically installed in a new environment on an mlflow run.

If you would like to run AttnGAN outside of MLFlow, install the dependencies using either
```
conda env create -f conda.yaml
```
or
```
pip install -r requirements.txt
```
into your chosen environment.

#### Data
The script `src/download.py` is available to automatically download data for the 'birds' and 'COCO' (broken; WIP) datasets. To download the 'birds' dataset, use `python src/download.py --dataset birds --all`. See `python src/download.py --help` for more control over what gets downloaded.

To download the data manually,

1. Download the preprocessed metadata (by the original AttnGAN authors) for [birds](https://drive.google.com/open?id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ) [coco](https://drive.google.com/open?id=1rSnbIGNDGZeHlsUlLdahj0RJ9oo6lgH9) and save them to `data/`.
2. Download the [birds](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) image data. Extract them to `data/birds/`.
3. Download [coco](http://cocodataset.org/#download) dataset and extract the images to `data/coco/`.

#### Training

- Pre-train DAMSM models:
  - For bird dataset: `python src/pretrain_DAMSM.py --cfg cfg/birds/DAMSM/bird.yml --gpu 0`
  - For coco dataset: `python src/pretrain_DAMSM.py --cfg cfg/coco/DAMSM/coco.yml --gpu 1`
 
- Train AttnGAN models:
  - For bird dataset: `python src/main.py --cfg cfg/birds/bird_attn2.yml --gpu 2`
  - For coco dataset: `python src/main.py --cfg cfg/coco/coco_attn2.yml --gpu 3`

- `*.yml` files are example configuration files for training/evaluation of the models.

Pass the number of the GPU (not the number of GPUs; multi-GPU has not been implemented yet) you want to use through the argument `--gpu`.

#### Pretrained Models
- [DAMSM for bird](https://drive.google.com/open?id=1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V). Download and save it to `DAMSMencoders/`
- [DAMSM for coco](https://drive.google.com/open?id=1zIrXCE9F6yfbEJIbNP5-YrEe2pZcPSGJ). Download and save it to `DAMSMencoders/`
- [AttnGAN for bird](https://drive.google.com/open?id=1lqNG75suOuR_8gjoEPYNp8VyT_ufPPig). Download and save it to `models/`
- [AttnGAN for coco](https://drive.google.com/open?id=1i9Xkg9nU74RAvkcqKE-rJYhjvzKAMnCi). Download and save it to `models/`

- [AttnDCGAN for bird](https://drive.google.com/open?id=19TG0JUoXurxsmZLaJ82Yo6O0UJ6aDBpg). Download and save it to `models/`
  - This is an variant of AttnGAN which applies the propsoed attention mechanisms to DCGAN framework. 

#### Generate images from text
- Run `python src/main.py --cfg cfg/birds/eval_bird.yml --gpu 0` to generate examples from captions in files listed in "data/birds/example_filenames.txt". Results are saved to `DAMSMencoders/`. 
- Change the `eval_*.yml` files to generate images from other pre-trained models. 
- Input your own sentence in "data/birds/example_captions.txt" if you wannt to generate images from customized sentences. 

#### Validation
- To generate images for all captions in the validation dataset, change B_VALIDATION to True in the eval_*.yml. and then run `python src/main.py --cfg cfg/birds/eval_bird.yml --gpu 0`.
- We compute inception score for models trained on birds using [StackGAN-inception-model](https://github.com/hanzhanggit/StackGAN-inception-model).
- We compute inception score for models trained on coco using [improved-gan/inception_score](https://github.com/openai/improved-gan/tree/master/inception_score).